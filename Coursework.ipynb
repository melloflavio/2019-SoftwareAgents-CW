{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydash as _\n",
    "import itertools\n",
    "from enum import Enum, unique\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Board movement\n",
    "General board movement functions and definitions.\n",
    "The board matrix is framed and padded with `NaN` for easier manipulation of movements without the need to take into account out of bounds indexes.\n",
    "The location map object maps string indexes to board coordinates. Given that the board has only 6 playable spaces, any possible board configuration can be translated into a single 6 character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Board & Movement constants\n",
    "BOARD_SIZE = (5,7) # NxN board\n",
    "BOARD_WIDTH = BOARD_SIZE[0]\n",
    "BOARD_HEIGHT = BOARD_SIZE[1]\n",
    "\n",
    "# Empty init to keep variables in global scope\n",
    "Q = [np.nan]\n",
    "R = [np.nan]\n",
    "\n",
    "LOCATION_MAP = {\n",
    "    0: (2,1),\n",
    "    1: (1,2),\n",
    "    2: (2,2),\n",
    "    3: (3,2),\n",
    "    4: (2,3),\n",
    "    5: (2,4),\n",
    "    6: (2,5),\n",
    "}\n",
    "\n",
    "@unique\n",
    "class ACTION(Enum):\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3\n",
    "    LEFT = 4\n",
    "    STAY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board <=> State String transitions \n",
    "Calculating and performing movements is easier done with a matrix than an encoded string, thus the project relies on translating state strings into boards back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_board():\n",
    "    empty_board = np.full(BOARD_SIZE, np.nan)\n",
    "    for loc_tuple in LOCATION_MAP.values():\n",
    "        empty_board[loc_tuple] = 0\n",
    "    return empty_board\n",
    "\n",
    "print('Empty board: \\n{}'.format(get_empty_board()))\n",
    "\n",
    "def get_board_for_state_string(state_string):\n",
    "    if(len(state_string) != len(LOCATION_MAP.values())):\n",
    "        raise ValueError('StateString size if invalid')\n",
    "    board = np.full(BOARD_SIZE, np.nan) # Start with empty board\n",
    "    state_tokens = list(state_string)\n",
    "    for location_idx, value in enumerate(state_tokens):\n",
    "        location_tuple = LOCATION_MAP[location_idx]\n",
    "        board[location_tuple] = value\n",
    "        \n",
    "    return board\n",
    "\n",
    "test_input = '1234567'\n",
    "print('\\n\\nBoard for state {}: \\n{}'.format(test_input, get_board_for_state_string(test_input)))\n",
    "\n",
    "\n",
    "def get_state_string_for_board(board):\n",
    "    \n",
    "    state_tokens = [np.nan]*len(LOCATION_MAP.values())\n",
    "    for (location_idx, location_tuple) in LOCATION_MAP.items():\n",
    "        state_tokens[location_idx] = str(int(board[location_tuple]))\n",
    "    \n",
    "    state_string = ''.join(state_tokens)\n",
    "    return state_string\n",
    "\n",
    "\n",
    "test_input = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "                       [np.nan, np.nan,   0.,   np.nan, np.nan, np.nan, np.nan],\n",
    "                       [np.nan,    1.,    0.,     0.,     0.,     0.,   np.nan],\n",
    "                       [np.nan, np.nan,   0.,   np.nan, np.nan, np.nan, np.nan],\n",
    "                       [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]])\n",
    "print('\\n\\nState for board: \\n{} \\n\\n{}'.format(test_input, get_state_string_for_board(test_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Transition & Reward\n",
    "\n",
    "By leveraging the state string to board transformations, we are able to calculate the possible movements for a given agent & execute said moves in a simple maner. Agents are able to move in the cardinal directions to adjacent open spaces. As an alternative, the agent always has the option of staying in place. All combined, there are 5 possible valid movements an agent can make, but depending on its position, only a subset may be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal States manually selected for 1,2 and 3 agent variations\n",
    "def get_terminal_state(num_agents):\n",
    "    return   '0000001' if num_agents is 1 \\\n",
    "        else '0000021' if num_agents is 2 \\\n",
    "        else '0000321' if num_agents is 3 \\\n",
    "        else ''\n",
    "print('Terminal State: {}'.format(get_terminal_state(1)))\n",
    "\n",
    "\n",
    "def get_random_state(num_agents):\n",
    "    state = get_terminal_state(num_agents) # Start with terminal state\n",
    "    state = list(state) # Shuffle requires an array as input\n",
    "    random.shuffle(state)\n",
    "    state = ''.join(state) # Back into a single string\n",
    "    return state\n",
    "\n",
    "print('Random State: {}'.format(get_random_state(1)))\n",
    "\n",
    "# Returns all possible states that can be reached given a number of agents in the board\n",
    "def get_possible_states(num_agents):\n",
    "    terminal_state = get_terminal_state(num_agents)\n",
    "    \n",
    "    possible_states = set(itertools.permutations(terminal_state)) # All UNIQUE permutations\n",
    "    possible_states = [''.join(state) for state in possible_states] # itertools returns arrays of chars, joining into complete strings\n",
    "    return possible_states\n",
    "\n",
    "print('Possible States:\\n  {}'.format(get_possible_states(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the destination is not valid (i.e. not an open space), the function returns the current position as next_state\n",
    "\n",
    "def move_agent(state_string, agent_id, action):\n",
    "    agent_location = state_string.find(str(agent_id))\n",
    "    (row, col) = LOCATION_MAP[agent_location]\n",
    "    board = get_board_for_state_string(state_string)\n",
    "    if (action == ACTION.UP and board[(row-1,col)] == 0):\n",
    "        board[(row-1,col)] = agent_id\n",
    "        board[(row,col)] = 0\n",
    "        return get_state_string_for_board(board)\n",
    "    elif (action == ACTION.RIGHT and board[(row,col+1)] == 0):\n",
    "        board[(row,col+1)] = agent_id\n",
    "        board[(row,col)] = 0\n",
    "        return get_state_string_for_board(board)\n",
    "    elif (action == ACTION.DOWN and board[(row+1,col)] == 0):\n",
    "        board[(row+1,col)] = agent_id\n",
    "        board[(row,col)] = 0\n",
    "        return get_state_string_for_board(board)\n",
    "    elif (action == ACTION.LEFT and board[(row,col-1)] == 0):\n",
    "        board[(row,col-1)] = agent_id\n",
    "        board[(row,col)] = 0\n",
    "        return get_state_string_for_board(board)\n",
    "    else: # Either ACTION.STAY or unnable to execute move\n",
    "        return get_state_string_for_board(board)\n",
    "\n",
    "print('Move from State: {} + {} => Resulting state: {}'.format('1000000', ACTION.DOWN, move_agent('1000000', 1, ACTION.DOWN)))\n",
    "\n",
    "def get_available_moves(state_string, agent_id):\n",
    "    moves = [move_agent(state_string, agent_id, action) for action in ACTION]\n",
    "    unique_moves = list(set(moves))\n",
    "    return unique_moves\n",
    "\n",
    "print('Available destinations from 0020010: {}'.format(get_available_moves('0020010', 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Matrix \n",
    "The transition matrix maps origin states (rows) into destination states (columns). Thus it is a NxN matrix, N being the number of possible states. Invalid transitions are represented as `NaN` values for a given tuple, while possible transitions are represented as `0`.\n",
    "\n",
    "Having the default value being `0` also serves a purpose of reutilizing the generated transition matrix as the basis for the R matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates NxN df representing all possible states. All initial values are set to 0. This can be used as the initial Q matrix, as well as the eligibility traces since both matrixes share same structure\n",
    "def generate_empty_state_matrix(num_agents):\n",
    "    possible_states = get_possible_states(num_agents)\n",
    "    \n",
    "#   Generate NxN Dataframe with all possible states as rows & columns and 0 as values\n",
    "    df = pd.DataFrame({'state': possible_states,\n",
    "                        **{state: 0 for state in possible_states}\n",
    "                       })\n",
    "    df.set_index('state', inplace=True)\n",
    "       \n",
    "    return df\n",
    "    \n",
    "generate_empty_state_matrix(num_agents=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_transition_matrix(agent_id, num_agents):\n",
    "    if (agent_id > num_agents):\n",
    "        raise ValueError('Invalid Agent_Id')\n",
    "    possible_states = get_possible_states(num_agents)\n",
    "    \n",
    "#   Generate blank NxN Dataframe with all possible states as rows & columns\n",
    "    df = pd.DataFrame({'state': possible_states,\n",
    "                        **{state: np.nan for state in possible_states}\n",
    "                       })\n",
    "    df.set_index('state', inplace=True)\n",
    "    \n",
    "#   Fill in possible transitions with 0\n",
    "    for state_string in possible_states:\n",
    "        available_moves = get_available_moves(state_string, agent_id)\n",
    "        df.loc[state_string, available_moves] = 0\n",
    "    \n",
    "    return df\n",
    "    \n",
    "generate_transition_matrix(agent_id=1, num_agents=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Parameter Setting\n",
    "The learning parameters are defined below as a globally scoped variable to enable ease of access by the methods lying in multiple layers of abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 10000,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.999,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.99,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "# Helper function to allow for agent id enumeration\n",
    "def get_agent_ids():\n",
    "    return range(1, EXPERIMENT['num_agents']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Matrix and reward function\n",
    "Given that each agent has slightly different transition functions, the reward function also difrs by agent. Although the same general rule applies. Agents are awarded 100 points for any transition that takes them to the desired terminal state, and they are penalized 100 points for any transition that takes them away from it. Any other transition is given no reward.\n",
    "\n",
    "For generating the R-matrix, the program simply starts with the transition matrix, and fills the relevant transitions with either `-100` or `100` accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(agent_id, num_agents):\n",
    "    reward = EXPERIMENT.get('return', 100)\n",
    "    penalty = EXPERIMENT.get('penalty', -100)\n",
    "#     Start with Transistion Matrix which already has 0 for all transitions\n",
    "    reward_matrix = generate_transition_matrix(agent_id, num_agents)\n",
    "    \n",
    "#     Set reward at possible transitions to terminal state\n",
    "    terminal_state = get_terminal_state(num_agents)\n",
    "    possible_origins = get_available_moves(terminal_state, agent_id) # Destinations & Origins are symmetric\n",
    "    reward_matrix.loc[terminal_state, possible_origins] = penalty # Moves going out of terminal state are penlized by 100\n",
    "    reward_matrix.loc[possible_origins, terminal_state] = reward # Moves going into terminal state are rewarded 100\n",
    "    return reward_matrix\n",
    "\n",
    "generate_reward_matrix(agent_id=1, num_agents=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tables(num_agents):\n",
    "    # Each agent has its own Q and R tables\n",
    "    Q = {agent_id: generate_transition_matrix(agent_id, num_agents) for agent_id in get_agent_ids()}\n",
    "    R = {agent_id: generate_reward_matrix(agent_id, num_agents) for agent_id in get_agent_ids()}\n",
    "    return (Q, R)\n",
    "    \n",
    "(Q, R) = initialize_tables(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon update policy\n",
    "The ε-greedy policy determines that the agent is to explore (i.e. take a random action) with ε probability, and exploit (i.e. take the action which has the highest expected return as per the Q-matrix) with (1-ε) probability. A strategy for gradual refinement is to have a gradually decreasing ε value. This means that when there is more uncertainty (i.e. at the beginning of the experiment, when not much is known regarding the environment), the agent takes action randomly to gather feedback and \"understanding\" of the surrounding environment. Gradually, the more the agent knows regarding the environment,less often it takes random actions. This effectively has the result of directing the random exploration to regions surrounding the current action policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_state(current_state, current_agent_id, epsilon):    \n",
    "    # exploit\n",
    "    if (random.random() >= epsilon):\n",
    "        current_q = Q[current_agent_id] # Get current agent's Q-matrix\n",
    "        max_expected_rewards = current_q.idxmax(axis=1) # Column index for max value in each row\n",
    "        return max_expected_rewards[current_state]\n",
    "        \n",
    "    # explore\n",
    "    else:\n",
    "        possible_moves = get_available_moves(current_state, current_agent_id)\n",
    "        return random.choice(possible_moves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon):\n",
    "    if (epsilon >= EXPERIMENT['epsilon']['decay_rate_threshold']):\n",
    "        decay_rate = EXPERIMENT['epsilon']['decay_rate_1']\n",
    "    else:\n",
    "        decay_rate = EXPERIMENT['epsilon']['decay_rate_2']\n",
    "    return epsilon * decay_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Matrix update\n",
    "\n",
    "\n",
    "After each step, the Q matrix is updated according to the earned return and current Q-Matrix according to the following formula\n",
    "$$Q(s_{t},a_{t}) = Q(s_{t},a_{t}) + α*[r_{t+1}+γ*max_{a} Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})]$$\n",
    "$$Q(s,a) = Q(s,a) + α*[r+γ*max_{a'} Q(s',a') - Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the RETURN in table R associated with transitioning from current to next state\n",
    "def calculate_earned_reward(current_state, next_state, current_agent_id):\n",
    "    current_r = R[current_agent_id] # Get current agent's R-matrix\n",
    "    earned_reward = current_r.loc[current_state, next_state]\n",
    "    return earned_reward\n",
    "\n",
    "def update_q_matrix(current_state, next_state, current_agent_id, earned_reward):\n",
    "    # Get alpha and gamma from experiment settings\n",
    "    alpha = EXPERIMENT['alpha']\n",
    "    gamma = EXPERIMENT['gamma']\n",
    "\n",
    "    # Get current agent's Q-matrix\n",
    "    current_q = Q[current_agent_id]\n",
    "\n",
    "    # Q(s,a)old\n",
    "    old_expected_return = current_q.loc[current_state, next_state]\n",
    "\n",
    "    # max a' Q(s',a')\n",
    "    possible_next_moves = get_available_moves(next_state, current_agent_id)\n",
    "    best_expected_return = max(current_q.loc[next_state, possible_next_moves])\n",
    "\n",
    "    # Q(s,a)new\n",
    "    new_expected_return = old_expected_return + alpha * (earned_reward + gamma * best_expected_return - old_expected_return)\n",
    "\n",
    "    # Update value in current agent's Q-matrix\n",
    "    current_q.loc[current_state, next_state] = new_expected_return\n",
    "    \n",
    "    change_difference = abs(new_expected_return-old_expected_return)\n",
    "    return change_difference\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Learning Episode\n",
    "Each learning episode starts with a random state. Agents then take turns performing actions, observeing the punishment/reward given by their respective R-Matrix and updating their respective Q-Matrix accordingly.\n",
    "\n",
    "An episode ends when either a predefined number of steps are taken(configurable in the experiment parameters), or the terminal state is reached. One detail that is important to stress is that the terminal state is only considered as reached when every single agent has perfomed an action that results in reaching that state, thus collecting its reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(epsilon):\n",
    "    # Episode starting values    \n",
    "    num_agents = EXPERIMENT['num_agents']\n",
    "    total_reward = 0 \n",
    "    agent_reward = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    current_state = get_random_state(num_agents)\n",
    "    state_history = [current_state]\n",
    "    did_episode_update = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    \n",
    "    while(not did_finish_episode(state_history)):\n",
    "        current_agent_id = (len(state_history) % num_agents) + 1 # agent_ids start at 1. Agents take turns performing actions\n",
    "        next_state = choose_next_state(current_state, current_agent_id, epsilon)\n",
    "        \n",
    "        earned_reward = calculate_earned_reward(current_state, next_state, current_agent_id) # Determine return from R matrix for the agent taking action\n",
    "        change_difference = update_q_matrix(current_state, next_state, current_agent_id, earned_reward) # Update Q Matrix for the agent taking action\n",
    "        \n",
    "        if(change_difference != 0):\n",
    "            did_episode_update[current_agent_id] += 1\n",
    "        \n",
    "        # Update values for next loop pass\n",
    "        state_history.append(next_state)\n",
    "        current_state = next_state\n",
    "        agent_reward[current_agent_id] += earned_reward\n",
    "        total_reward += earned_reward\n",
    "        \n",
    "    did_reach_terminal = state_history[-1] == get_terminal_state(num_agents)\n",
    "    \n",
    "    return ({\n",
    "        'epsilon': epsilon,\n",
    "        'total_steps':len(state_history), \n",
    "        **{('reward_'+str(agent_id)): agent_reward[agent_id] for agent_id in get_agent_ids()},\n",
    "        'reward_total':total_reward,\n",
    "        'final_state': current_state, \n",
    "        'did_reach_terminal':did_reach_terminal,\n",
    "        **{('did_update_q_'+str(agent_id)): did_episode_update[agent_id] for agent_id in get_agent_ids()},        \n",
    "    })\n",
    "\n",
    "def did_finish_episode(state_history):\n",
    "    current_step = len(state_history)\n",
    "    terminal_state = get_terminal_state(EXPERIMENT['num_agents'])\n",
    "    recent_history = state_history[-EXPERIMENT['num_agents']:]     # Ensures terminal state is reached only after ALL agents took one action after reaching it & gained their reward\n",
    "    \n",
    "    if (current_step > 1 and all(state == terminal_state for state in recent_history)):\n",
    "        return True\n",
    "    elif (current_step > EXPERIMENT['max_steps']): # Agent timeout. Finish episode by exhaustion\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "run_episode(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "An experiment is defined as a collection of learning episodes while using a defined set of parameters. After each learning episode, it's results are stored for later analysis, and the epsilon value is updated according to the epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop\n",
    "\n",
    "# Empty init to keep variables in global scope\n",
    "# Q = np.nan\n",
    "# R = np.nan\n",
    "\n",
    "def perform_experiment():\n",
    "    global Q\n",
    "    global R\n",
    "    (Q, R) = initialize_tables(EXPERIMENT['num_agents'])\n",
    "    \n",
    "    epsilon = EXPERIMENT['epsilon']['initial_value']\n",
    "    results = []\n",
    "    \n",
    "    for current_episode in range(EXPERIMENT['num_episodes']):\n",
    "        episode_result = run_episode(epsilon)\n",
    "        results.append({'episode': current_episode, **episode_result})\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "        # Progress report\n",
    "        if (current_episode%(EXPERIMENT['num_episodes']/10) == 0):\n",
    "            progress = current_episode/EXPERIMENT['num_episodes']\n",
    "            progress_string = '{:02d}%  '.format(int(progress*100))\n",
    "            print(progress_string, end='')\n",
    "    print('100%')\n",
    "    return results\n",
    "\n",
    "perform_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING_WINDOW=100\n",
    "\n",
    "# ColorBrewer 8-color qualitative scale\n",
    "COLOR = [\n",
    "    '#1b9e77',\n",
    "    '#d95f02',\n",
    "    '#7570b3',\n",
    "    '#e7298a',\n",
    "    '#66a61e',\n",
    "    '#e6ab02',\n",
    "    '#a6761d',\n",
    "    '#666666',\n",
    "]\n",
    "\n",
    "def show_results(experiment_results):\n",
    "    df = process_results(experiment_results)\n",
    "    \n",
    "    # Figure 1: Win % (rolling) and Epsilon vs Learning Episode\n",
    "    plot_win_vs_epsilon(df, 1)\n",
    "    \n",
    "    # Figure 2: Steps vs Episode\n",
    "    plot_total_steps_rolling(df, 2)\n",
    "    \n",
    "    # Figure 3: Q Update vs Episode\n",
    "    plot_did_change_q(df, 3)\n",
    "    \n",
    "\n",
    "    \n",
    "def process_results(experiment_results):\n",
    "    df = pd.DataFrame.from_records(experiment_results)\n",
    "    for agent_id in get_agent_ids():\n",
    "        agent_id = str(agent_id)\n",
    "        df['rwd_'+agent_id] = df['reward_'+agent_id]/df['total_steps']\n",
    "    df['rwd_tot'] = df['reward_total']/df['total_steps']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_episode_epsilon_change(df):\n",
    "    decay_threshold = EXPERIMENT['epsilon']['decay_rate_threshold']\n",
    "    episode = df[df.epsilon >= decay_threshold].head(1).episode.values[0]\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_win_vs_epsilon(df, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = df['episode']\n",
    "    y1 = df['did_reach_terminal'].rolling(ROLLING_WINDOW).sum()/ROLLING_WINDOW\n",
    "    y2 = df['epsilon']\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax_color_1 = COLOR[5]\n",
    "    ax_color_2 = COLOR[0]\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, y1, color=ax_color_1)\n",
    "    ax2.plot(x, y2, color=ax_color_2)\n",
    "\n",
    "    ax1.set_xlabel('Episodes', fontsize=12)\n",
    "    ax1.set_ylabel('Win % Rolling Average ({})'.format(ROLLING_WINDOW), color=ax_color_1, fontsize=12)\n",
    "    ax2.set_ylabel('Epsilon', color=ax_color_2, fontsize=12)\n",
    "    plt.title('Win % Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "    \n",
    "    # Vertical Line where Epsilon Decay Changes\n",
    "    ax1.axvline(x=get_episode_epsilon_change(df), linestyle='--', linewidth=1, color='gray')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_win_vs_epsilon(process_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_steps_rolling(df, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = df['episode']\n",
    "    y1 = df['total_steps'].rolling(ROLLING_WINDOW).mean()\n",
    "    y2 = df['epsilon']\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax_color_1 = COLOR[3]\n",
    "    ax_color_2 = COLOR[0]\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, y1, color=ax_color_1)\n",
    "    ax2.plot(x, y2, color=ax_color_2)\n",
    "\n",
    "    ax1.set_xlabel('Episodes', fontsize=12)\n",
    "    ax1.set_ylabel('Total Steps - Rolling Average ({})'.format(ROLLING_WINDOW), color=ax_color_1, fontsize=12)\n",
    "    ax2.set_ylabel('Epsilon', color=ax_color_2, fontsize=12)\n",
    "    plt.title('Total Steps - Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "\n",
    "    # Vertical Line where Epsilon Decay Changes\n",
    "    ax1.axvline(x=get_episode_epsilon_change(df), linestyle='--', linewidth=1, color='gray')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# plot_total_steps_rolling(process_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_did_change_q(df, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = df['episode']\n",
    "    y2 = df['epsilon']\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax_color_2 = COLOR[0]\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    agent_labels =[]\n",
    "    for agent_id in get_agent_ids():\n",
    "        color = COLOR[agent_id]\n",
    "        agent_id = str(agent_id)\n",
    "        agent_labels.append('Agent {}'.format(agent_id))\n",
    "        y = (df['did_update_q_'+agent_id]/df['total_steps']).rolling(ROLLING_WINDOW).mean()\n",
    "        ax1.plot(x, y, color=color)\n",
    "    ax1.legend(agent_labels)\n",
    "    ax2.plot(x, y2, color=ax_color_2)\n",
    "\n",
    "    ax1.set_xlabel('Episodes', fontsize=12)\n",
    "    ax1.set_ylabel('Q Updates/Steps'.format(ROLLING_WINDOW), fontsize=12)\n",
    "    ax2.set_ylabel('Epsilon', color=ax_color_2, fontsize=12)\n",
    "    plt.title('Q Updates per Step - Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "\n",
    "    # Vertical Line where Epsilon Decay Changes\n",
    "    ax1.axvline(x=get_episode_epsilon_change(df), linestyle='--', linewidth=1, color='gray')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# plot_did_change_q(process_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results for multiple experiments with varying params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_multi(all_results, labels):    \n",
    "    #     Figure 1: Win % (rolling) and Epsilon vs Learning Episode\n",
    "    plot_win_vs_epsilon_multi(all_results, labels, 1)\n",
    "    #     Figure 2: Steps vs Episode\n",
    "    plot_total_steps_multi(all_results, labels, 2)\n",
    "    #     Figure 3: Q Update vs Episode\n",
    "    plot_did_change_q_multi(all_results, labels, 3)\n",
    "\n",
    "# show_results_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_win_vs_epsilon_multi(all_dfs, labels, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = all_dfs[0]['episode']\n",
    "    \n",
    "    for index, df in enumerate(all_dfs):\n",
    "        y = df['did_reach_terminal'].rolling(ROLLING_WINDOW).sum()/ROLLING_WINDOW\n",
    "        plt.plot(x, y, color=COLOR[index])\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel('Episodes', fontsize=12)\n",
    "    plt.ylabel('Win % Rolling Average ({})'.format(ROLLING_WINDOW), fontsize=12)\n",
    "    plt.title('Win % Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_win_vs_epsilon_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_steps_multi(all_dfs, labels, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = all_dfs[0]['episode']\n",
    "    \n",
    "    for index, df in enumerate(all_dfs):\n",
    "        y = df['total_steps'].rolling(ROLLING_WINDOW).mean()\n",
    "        plt.plot(x, y, color=COLOR[index])\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel('Episodes', fontsize=12)\n",
    "    plt.ylabel('Total Steps - Rolling Average ({})'.format(ROLLING_WINDOW), fontsize=12)\n",
    "    plt.title('Total Steps - Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# plot_total_steps_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_did_change_q_multi(all_dfs, labels, figure_num=1):\n",
    "    plt.figure(figure_num); \n",
    "    x = all_dfs[0]['episode']\n",
    "    \n",
    "    linestyles = {'1':'-', '2':':', '3':'--'}\n",
    "    \n",
    "    agent_labels =[]\n",
    "    for index, df in enumerate(all_dfs):\n",
    "        for agent_id in get_agent_ids():\n",
    "            color = COLOR[agent_id]\n",
    "            agent_id = str(agent_id)\n",
    "            agent_labels.append('{}-{}'.format(agent_id, labels[index]))\n",
    "            y = (df['did_update_q_'+agent_id]/df['total_steps']).rolling(ROLLING_WINDOW).mean()\n",
    "            plt.plot(x, y, color=COLOR[index], ls=linestyles[agent_id])\n",
    "\n",
    "#     plt.legend(labels)\n",
    "\n",
    "    \n",
    "    plt.legend(agent_labels)\n",
    "    plt.xlabel('Episodes', fontsize=12)\n",
    "    plt.ylabel('Q Updates/Steps'.format(ROLLING_WINDOW), fontsize=12)\n",
    "    plt.title('Q Updates per Step - Rolling Average ({}) x Episodes'.format(ROLLING_WINDOW), fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# plot_did_change_q_multi(all_results, alphas_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP 1.1: 1 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "results = perform_experiment()\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP 1.2: 2 Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 2,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "results = perform_experiment()\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 2,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "results = perform_experiment()\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 3 - Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "alpha_results = []\n",
    "alphas_to_test = [a/10 for a in range(0, 11, 2)]\n",
    "for alpha in alphas_to_test:\n",
    "    EXPERIMENT['alpha'] = alpha\n",
    "    print('alpha={} - '.format(alpha), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    alpha_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(alpha_results, alphas_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 4 - Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "gamma_results = []\n",
    "gammas_to_test = [g/10 for g in range(1, 10, 2)]\n",
    "for gamma in gammas_to_test:\n",
    "    EXPERIMENT['gamma'] = gamma\n",
    "    print('gamma={} - '.format(gamma), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    gamma_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(gamma_results, gammas_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP 5 - Episode Duration (MaxSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 3000,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "max_steps_results = []\n",
    "max_steps_to_test = [4, 6, 8, 10, 30, 60, 100]\n",
    "for max_steps in max_steps_to_test:\n",
    "    EXPERIMENT['max_steps'] = max_steps\n",
    "    print('max_steps={} - '.format(max_steps), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    max_steps_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(max_steps_results, max_steps_to_test)\n",
    "\n",
    "EXPERIMENT['epsilon'] = {\n",
    "        'initial_value': 1,\n",
    "        'decay_rate_1': 1,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 1,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0,\n",
    "    }\n",
    "\n",
    "max_steps_random_results = []\n",
    "for max_steps in max_steps_to_test:\n",
    "    EXPERIMENT['max_steps'] = max_steps\n",
    "    print('max_steps={} - '.format(max_steps), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    max_steps_random_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(max_steps_random_results, max_steps_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 6 - Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 6.1 - Initial Epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 400,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "epsilon_results = []\n",
    "\n",
    "epsilon_to_test = [0, 0.2, 0.5, 0.9, 0.999, 1]\n",
    "for epsilon in epsilon_to_test:\n",
    "    EXPERIMENT['epsilon']['initial_value'] = epsilon\n",
    "    print('initial_value={} - '.format(epsilon), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    epsilon_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 6.2 - Epsilon Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 400,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "epsilon_results = []\n",
    "epsilon_to_test = [0.9, 0.95, 0.99, 0.995, 0.997, 0.998]\n",
    "for epsilon in epsilon_to_test:\n",
    "    EXPERIMENT['epsilon']['decay_rate_threshold'] = epsilon\n",
    "    print('decay_rate_threshold={} - '.format(epsilon), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    epsilon_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Decay Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 1,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.8, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "\n",
    "epsilon_results = []\n",
    "\n",
    "epsilon_to_test = [(0.99995, 0.9995), (0.995, 0.95), (0.99, 0.9), (0.9, 0.8), (0.79,0.7), (0.59,0.5) ]\n",
    "for epsilon in epsilon_to_test:\n",
    "    EXPERIMENT['epsilon']['decay_rate_1'] = epsilon[0]\n",
    "    EXPERIMENT['epsilon']['decay_rate_2'] = epsilon[1]\n",
    "    print('decay rate={}-{} - '.format(epsilon[0], epsilon[1]), end='')\n",
    "    instance_results = perform_experiment()\n",
    "    epsilon_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(epsilon_results, epsilon_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - TD(λ) Learning with eligibility traces\n",
    "Implementation of TD(λ)with eligibility traces. The episodes and experiments follow the same structure as the previously implementation of Q-Learning with minor changes to account for the difference in the Q-Matrix update function and need to keep track of the eligibility trace matrixes (E) for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eligibility Traces - TD(lambda)\n",
    "def update_q_matrix_td(current_state, next_state, current_agent_id, earned_reward):\n",
    "    global Q\n",
    "    global E\n",
    "    # Get alpha and gamma from experiment settings\n",
    "    alpha = EXPERIMENT['alpha']\n",
    "    gamma = EXPERIMENT['gamma']\n",
    "    lamb = EXPERIMENT['lambda']\n",
    "    \n",
    "    eligibility = E[current_agent_id]\n",
    "\n",
    "    # Get current agent's Q-matrix\n",
    "    current_q = Q[current_agent_id]\n",
    "\n",
    "    # Q(s,a)old\n",
    "    old_expected_return = current_q.loc[current_state, next_state]\n",
    "    \n",
    "    # max a' Q(s',a')\n",
    "    possible_next_moves = get_available_moves(next_state, current_agent_id)\n",
    "    best_expected_return = max(current_q.loc[next_state, possible_next_moves])\n",
    "    \n",
    "    # δ\n",
    "    delta = earned_reward + (gamma * best_expected_return) - old_expected_return\n",
    "    \n",
    "    # Decay Eligibility & update current trace\n",
    "    eligibility = eligibility*gamma*lamb\n",
    "    eligibility.loc[current_state, next_state] = 1 # Replacing Traces technique\n",
    "    E[current_agent_id] = eligibility\n",
    "    \n",
    "    Q[current_agent_id] = current_q + (alpha * delta * eligibility)\n",
    "\n",
    "\n",
    "\n",
    "# update_q_matrix_td(\"0001000\", \"0000010\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# TD Learning\n",
    "def run_episode_td(epsilon):\n",
    "    # Episode starting values\n",
    "    num_agents = EXPERIMENT['num_agents']\n",
    "    total_reward = 0 \n",
    "    agent_reward = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    current_state = get_random_state(num_agents)\n",
    "    state_history = [current_state]\n",
    "    did_episode_update = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    \n",
    "    # Initialize new eligibility trae every single episode\n",
    "    global E\n",
    "    E = {agent_id: generate_empty_state_matrix(num_agents) for agent_id in get_agent_ids()}\n",
    "    \n",
    "    # Run episode\n",
    "    while(not did_finish_episode(state_history)):\n",
    "        current_agent_id = (len(state_history) % num_agents) + 1 # agent_ids start at 1. Agents take turns performing actions\n",
    "        next_state = choose_next_state(current_state, current_agent_id, epsilon)\n",
    "        \n",
    "        earned_reward = calculate_earned_reward(current_state, next_state, current_agent_id) # Determine return from R matrix for the agent taking action\n",
    "        \n",
    "        old_Q = copy.deepcopy(Q)\n",
    "        \n",
    "        update_q_matrix_td(current_state, next_state, current_agent_id, earned_reward) # Update Q Matrix for the agent taking action\n",
    "        \n",
    "        # Q-Matrix update calculated as any difference between original and resulting matrixes\n",
    "        if (not Q[current_agent_id].equals(old_Q)):\n",
    "            did_episode_update[current_agent_id] += 1\n",
    "        \n",
    "        # Update values for next loop pass\n",
    "        state_history.append(next_state)\n",
    "        current_state = next_state\n",
    "        agent_reward[current_agent_id] += earned_reward\n",
    "        total_reward += earned_reward\n",
    "        \n",
    "    did_reach_terminal = state_history[-1] == get_terminal_state(num_agents)\n",
    "    \n",
    "    return ({\n",
    "        'epsilon': epsilon,\n",
    "        'total_steps':len(state_history), \n",
    "        **{('reward_'+str(agent_id)): agent_reward[agent_id] for agent_id in get_agent_ids()},\n",
    "        'reward_total':total_reward,\n",
    "        'final_state': current_state, \n",
    "        'did_reach_terminal':did_reach_terminal,\n",
    "        **{('did_update_q_'+str(agent_id)): did_episode_update[agent_id] for agent_id in get_agent_ids()}, \n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop\n",
    "\n",
    "# Empty init to keep variables in global scope\n",
    "Q = np.nan\n",
    "R = np.nan\n",
    "\n",
    "def perform_experiment_td():\n",
    "    global Q\n",
    "    global R\n",
    "    (Q, R) = initialize_tables(EXPERIMENT['num_agents'])\n",
    "    \n",
    "    epsilon = EXPERIMENT['epsilon']['initial_value']\n",
    "    results = []\n",
    "    \n",
    "    for current_episode in range(EXPERIMENT['num_episodes']):\n",
    "        episode_result = run_episode_td(epsilon)\n",
    "        results.append({'episode': current_episode, **episode_result})\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "        # Progress report\n",
    "        if (current_episode%(EXPERIMENT['num_episodes']/10) == 0):\n",
    "            progress = current_episode/EXPERIMENT['num_episodes']\n",
    "            progress_string = '{:02d}%  '.format(int(progress*100))\n",
    "            print(progress_string, end='')\n",
    "    print('100%')\n",
    "    return results\n",
    "\n",
    "# perform_experiment_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 2,        # ∈ {1,2,3}\n",
    "    'num_episodes': 5000,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "    'lambda': 0.2, # Lambda in TD(lambda) eligibility traces\n",
    "}\n",
    "\n",
    "lambda_results = []\n",
    "\n",
    "lambda_to_test = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for lamb in lambda_to_test:\n",
    "    EXPERIMENT['lambda'] = lamb\n",
    "\n",
    "    print('Lambda={} - '.format(lamb), end='')\n",
    "    instance_results = perform_experiment_td()\n",
    "    lambda_results.append(process_results(instance_results))\n",
    "\n",
    "show_results_multi(lambda_results, lambda_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Hyperstate\n",
    "Rewritten implementation of Q-Learning while taking into account the next hyperstate (all states resulting from the other agent's moves), rather than simply next state, when calculating the best expected return for the future action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the best expected return for a given state and agent\n",
    "def get_max_expected_return(next_state, current_agent_id):\n",
    "    current_q = Q[current_agent_id]\n",
    "    possible_next_moves = get_available_moves(next_state, current_agent_id)\n",
    "    best_expected_return = max(current_q.loc[next_state, possible_next_moves])\n",
    "    \n",
    "    return best_expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all the possible states (i.e. hyperstate) a given agent may find herself in after taking\n",
    "# an action leading to \"next_state\". In other words, from the given next state, calculates all posible \n",
    "# states resulting from an action of the other agent/player.\n",
    "def get_next_hyperstate(next_state, current_agent):\n",
    "    num_agents = EXPERIMENT['num_agents']\n",
    "    next_agent_id = (current_agent % num_agents) + 1\n",
    "    hyperstate = get_available_moves(next_state, next_agent_id)\n",
    "    \n",
    "    return hyperstate\n",
    "\n",
    "get_next_hyperstate(\"0000201\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the best expeted return for a given state and agent while taking into account the hyperstate possibilities\n",
    "# from moves taken by the other agent\n",
    "def get_max_expected_return_hyperstate(next_state, current_agent_id):\n",
    "    next_hyperstate = get_next_hyperstate(next_state, current_agent_id)\n",
    "    best_returns = [get_max_expected_return(state, current_agent_id) for state in next_hyperstate]\n",
    "    best_expected_return = max(best_returns)\n",
    "    \n",
    "    return best_expected_return\n",
    "\n",
    "get_max_expected_return_for_hyperstate(\"0000201\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates Q-Matrix using Q-Learning technique with a modification to take into account future \n",
    "# hyperstates (i.e. states resulting from the other player's actions) rather than the single \n",
    "# state resulting from a given agent's last action\n",
    "def update_q_matrix_hyperstate(current_state, next_state, current_agent_id, earned_reward):\n",
    "    # Get alpha and gamma from experiment settings\n",
    "    alpha = EXPERIMENT['alpha']\n",
    "    gamma = EXPERIMENT['gamma']\n",
    "\n",
    "    # Get current agent's Q-matrix\n",
    "    current_q = Q[current_agent_id]\n",
    "\n",
    "    # Q(s,a)old\n",
    "    old_expected_return = current_q.loc[current_state, next_state]\n",
    "\n",
    "    # max a' Q(hs',a')\n",
    "    best_expected_return = get_max_expected_return_hyperstate(next_state, current_agent_id)\n",
    "\n",
    "    # Q(s,a)new\n",
    "    new_expected_return = old_expected_return + alpha * (earned_reward + gamma * best_expected_return - old_expected_return)\n",
    "\n",
    "    # Update value in current agent's Q-matrix\n",
    "    current_q.loc[current_state, next_state] = new_expected_return\n",
    "    \n",
    "    change_difference = abs(new_expected_return-old_expected_return)\n",
    "    return change_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment_hyperstate():\n",
    "    global Q\n",
    "    global R\n",
    "    (Q, R) = initialize_tables(EXPERIMENT['num_agents'])\n",
    "    \n",
    "    epsilon = EXPERIMENT['epsilon']['initial_value']\n",
    "    results = []\n",
    "    \n",
    "    for current_episode in range(EXPERIMENT['num_episodes']):\n",
    "        episode_result = run_episode_hyperstate(epsilon)\n",
    "        results.append({'episode': current_episode, **episode_result})\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "        # Progress report\n",
    "        if (current_episode%(EXPERIMENT['num_episodes']/10) == 0):\n",
    "            progress = current_episode/EXPERIMENT['num_episodes']\n",
    "            progress_string = '{:02d}%  '.format(int(progress*100))\n",
    "            print(progress_string, end='')\n",
    "    print('100%')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_hyperstate(epsilon):\n",
    "    # Episode starting values    \n",
    "    num_agents = EXPERIMENT['num_agents']\n",
    "    total_reward = 0 \n",
    "    agent_reward = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    current_state = get_random_state(num_agents)\n",
    "    state_history = [current_state]\n",
    "    did_episode_update = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    \n",
    "    while(not did_finish_episode(state_history)):\n",
    "        current_agent_id = (len(state_history) % num_agents) + 1 # agent_ids start at 1. Agents take turns performing actions\n",
    "        next_state = choose_next_state(current_state, current_agent_id, epsilon)\n",
    "        \n",
    "        earned_reward = calculate_earned_reward(current_state, next_state, current_agent_id) # Determine return from R matrix for the agent taking action\n",
    "        change_difference = update_q_matrix_hyperstate(current_state, next_state, current_agent_id, earned_reward) # Update Q Matrix for the agent taking action\n",
    "        \n",
    "        if(change_difference != 0):\n",
    "            did_episode_update[current_agent_id] += 1\n",
    "        \n",
    "        # Update values for next loop pass\n",
    "        state_history.append(next_state)\n",
    "        current_state = next_state\n",
    "        agent_reward[current_agent_id] += earned_reward\n",
    "        total_reward += earned_reward\n",
    "        \n",
    "    did_reach_terminal = state_history[-1] == get_terminal_state(num_agents)\n",
    "    \n",
    "    return ({\n",
    "        'epsilon': epsilon,\n",
    "        'total_steps':len(state_history), \n",
    "        **{('reward_'+str(agent_id)): agent_reward[agent_id] for agent_id in get_agent_ids()},\n",
    "        'reward_total':total_reward,\n",
    "        'final_state': current_state, \n",
    "        'did_reach_terminal':did_reach_terminal,\n",
    "        **{('did_update_q_'+str(agent_id)): did_episode_update[agent_id] for agent_id in get_agent_ids()},        \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = {\n",
    "    'num_agents': 2,        # ∈ {1,2,3}\n",
    "    'num_episodes': 2500,    # Number of episodes the experiment lasts\n",
    "    'max_steps': 30,       # Max steps per episode\n",
    "    'epsilon': {\n",
    "        'initial_value': 0.999,\n",
    "        'decay_rate_1': 0.9995,  # Decay if e >= threshold\n",
    "        'decay_rate_2': 0.995,   # Decay if e < threshold\n",
    "        'decay_rate_threshold': 0.5,\n",
    "    },\n",
    "    'alpha': 0.5, # learning rate\n",
    "    'gamma': 0.5, # dicount factor\n",
    "    'return': 100, # Return for moving into terminal state\n",
    "    'penalty': -100, # Punishment for leaving terminal state\n",
    "}\n",
    "results = perform_experiment_hyperstate()\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n",
    "Helper Function used to debug a Q-Matrix present the agents' decisions in a simpler way. Effectively runs a simulated episode with totally greedy policy (i.e. epsilon = 0) to make agents exploit every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to test the current Q-Matrix. Starts from random state and takes greedy steps, printing out results\n",
    "def debug_q():\n",
    "    # Episode starting values\n",
    "    num_agents = 2\n",
    "    total_reward = 0 \n",
    "    agent_reward = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    current_state = get_random_state(num_agents)\n",
    "    state_history = [current_state]\n",
    "    did_episode_update = {agent_id: 0 for agent_id in get_agent_ids()}\n",
    "    \n",
    "    while(not did_finish_episode(state_history)):\n",
    "        current_agent_id = (len(state_history) % num_agents) + 1 # agent_ids start at 1. Agents take turns performing actions\n",
    "        next_state = choose_next_state(current_state, current_agent_id, 0)\n",
    "        \n",
    "        earned_reward = calculate_earned_reward(current_state, next_state, current_agent_id) # Determine return from R matrix for the agent taking action\n",
    "        \n",
    "        print(\"{}: {}={}  ${}\".format(current_agent_id, current_state, next_state, earned_reward))\n",
    "        \n",
    "        # Update values for next loop pass\n",
    "        state_history.append(next_state)\n",
    "        current_state = next_state\n",
    "        agent_reward[current_agent_id] += earned_reward\n",
    "        total_reward += earned_reward\n",
    "        \n",
    "        \n",
    "    did_reach_terminal = state_history[-1] == get_terminal_state(num_agents)\n",
    "\n",
    "# debug_q()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
