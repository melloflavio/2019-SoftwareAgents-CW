\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Domain and Task}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Playing Field Arrangement}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual representation of the playing field}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Playing Rules}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Task Goal}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}State Transition and Reward Functions}{2}\protected@file@percent }
\newlabel{sec:transition}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Playing field with location ids}}{2}\protected@file@percent }
\newlabel{fig:field_indexed}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces State string with equivalent playing field representation}}{2}\protected@file@percent }
\newlabel{fig:field_mapping}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning Policy}{3}\protected@file@percent }
\newlabel{sec:learning_policy}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Graphical Representation \& R Matrix}{3}\protected@file@percent }
\newlabel{sec:r_matrix}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Base Transition Graph}}{3}\protected@file@percent }
\newlabel{fig:transition_graph}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces R Matrix generated for experiment with a single agent}}{4}\protected@file@percent }
\newlabel{fig:r_matrix}{{5}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Q-Learning Parameters}{4}\protected@file@percent }
\newlabel{sec:params}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discount Factor - $\gamma $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Learning Rate - $\alpha $}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Epsilon - $\epsilon $}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Experiment duration - (\texttt  {num\_episodes} and \texttt  {max\_steps})}{5}\protected@file@percent }
\newlabel{sec:params:duration}{{5.4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Reward/Penalty}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Number of Agents - (\texttt  {num\_agents})}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Q-Matrix Update Cycle}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Initial Results}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Single agent}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Two agents}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Percentage of episodes able to reach terminal state in single agent experiment}}{8}\protected@file@percent }
\newlabel{fig:exp1:win_percent}{{6}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Steps taken per episode in single agent experiment}}{8}\protected@file@percent }
\newlabel{fig:exp1:steps}{{7}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Further Tests}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Experiment duration}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}$\alpha $ values}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of updates to Q Matrix per step taken in single agent experiment}}{9}\protected@file@percent }
\newlabel{fig:exp1:updates}{{8}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Percentage of episodes able to reach terminal state in two-agent experiment}}{9}\protected@file@percent }
\newlabel{fig:exp2:win_percent}{{9}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}$\gamma $ values}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Maximum Episode duration - \texttt  {max\_steps}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}$\epsilon $ decay}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Steps taken per episode in two-agent experiment}}{10}\protected@file@percent }
\newlabel{fig:exp2:steps}{{10}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Number of updates to Q Matrix per step taken in two-agent experiment}}{10}\protected@file@percent }
\newlabel{fig:exp2:updates}{{11}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Quantitative Analysis}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Qualitative Analysis}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Maximum Episode duration - \texttt  {max\_steps}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Decay Rate - $\epsilon $}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Advanced Reinforcement Learning Techniques}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Q-learning and Psychological Error Correction Models}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Error Correction Implementation Proposal}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Q-Matrix update code}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Win \% progression with varying $\alpha $ - Single agent}}{11}\protected@file@percent }
\newlabel{fig:exp3:win_percent}{{12}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Win \% progression with varying $\alpha $ - Two agents}}{11}\protected@file@percent }
\newlabel{fig:exp3:win_percent_two}{{13}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Win \% progression with varying $\gamma $ - Single agent}}{11}\protected@file@percent }
\newlabel{fig:exp4:win_percent}{{14}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Win \% progression with varying $\gamma $ - Two agents}}{11}\protected@file@percent }
\newlabel{fig:exp4:win_percent_two}{{15}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Win \% progression with varying \texttt  {max\_steps} - Single agent}}{12}\protected@file@percent }
\newlabel{fig:exp5:win_percent}{{16}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Win \% progression with varying \texttt  {max\_steps} - Two agents}}{12}\protected@file@percent }
\newlabel{fig:exp5:win_percent_two}{{17}{12}}
