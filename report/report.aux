\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Domain and Task}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Playing Field Arrangement}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual representation of the playing field}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Playing Rules}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Task Goal}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}State Transition and Reward Functions}{1}\protected@file@percent }
\newlabel{sec:transition}{{2}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Playing field with location ids}}{2}\protected@file@percent }
\newlabel{fig:field_indexed}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces State string with equivalent playing field representation}}{2}\protected@file@percent }
\newlabel{fig:field_mapping}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning Policy}{2}\protected@file@percent }
\newlabel{sec:learning_policy}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Graphical Representation \& R Matrix}{2}\protected@file@percent }
\newlabel{sec:r_matrix}{{4}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Base Transition Graph}}{3}\protected@file@percent }
\newlabel{fig:transition_graph}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces R Matrix generated for experiment with a single agent}}{3}\protected@file@percent }
\newlabel{fig:r_matrix}{{5}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Q-Learning Parameters}{3}\protected@file@percent }
\newlabel{sec:params}{{5}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discount Factor - $\gamma $}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Learning Rate - $\alpha $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Epsilon - $\epsilon $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Experiment duration - (\texttt  {num\_episodes} and \texttt  {max\_steps})}{4}\protected@file@percent }
\newlabel{sec:params:duration}{{5.4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Reward/Penalty}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Number of Agents - (\texttt  {num\_agents})}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Q-Matrix Update Cycle}{4}\protected@file@percent }
\newlabel{eq:q_update}{{2}{4}}
\newlabel{lst:q_update_code}{{1}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Implementation code for Q-Matrix Update}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Initial Results}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Single agent}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Percentage of episodes able to reach terminal state in single agent experiment}}{5}\protected@file@percent }
\newlabel{fig:exp1:win}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Two agents}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Two agents experiment - Win \% (left) and steps per episode (right)}}{6}\protected@file@percent }
\newlabel{fig:exp2:win}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of updates to Q Matrix per step taken in two-agent experiment}}{6}\protected@file@percent }
\newlabel{fig:exp2:updates}{{8}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Further Tests}{6}\protected@file@percent }
\newlabel{sec:further_tests}{{8}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Experiment duration}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}$\alpha $ values}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}$\gamma $ values}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Maximum Episode duration - \texttt  {max\_steps}}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Win \% progression with varying $\alpha $ - Single agent(left) and Two agents (right)}}{7}\protected@file@percent }
\newlabel{fig:exp3}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Win \% progression with varying $\gamma $ - Single agent(left) and Two agents(right)}}{7}\protected@file@percent }
\newlabel{fig:exp4}{{10}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}$\epsilon $ decay}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Quantitative Analysis}{7}\protected@file@percent }
\newlabel{sec:quant_analysis}{{9}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Qualitative Analysis - Single Agent}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Maximum Episode duration - \texttt  {max\_steps}}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Win \% progression with varying \texttt  {max\_steps} - Single agent(left) and Two agents (right)}}{8}\protected@file@percent }
\newlabel{fig:exp5}{{11}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Single agent Win \% progression: varying $\epsilon $ decay change threshold(left) and varying $\epsilon $ decay rate(right)}}{8}\protected@file@percent }
\newlabel{fig:exp6}{{12}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}$\alpha $ and $\gamma $}{8}\protected@file@percent }
\newlabel{sec:quali:gamma}{{10.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Decay Rate - $\epsilon $}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Qualitative Analysis - Two Agents}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Advanced Reinforcement Learning Techniques}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}TD($\lambda $)-learning}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Win \% progression vs $\lambda $ - Two agents}}{9}\protected@file@percent }
\newlabel{fig:expII:winning}{{13}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Hyperstate Q-Learning}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Win \% progression - Hyperstate Q-Learning - Two agents}}{10}\protected@file@percent }
\newlabel{fig:expIII:winning}{{14}{10}}
\newlabel{lst:hyperstate}{{2}{11}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Implementation code for Hyperstate - Best expected return calculation}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Q-learning and Psychological Error Correction Models}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Error Correction Implementation Proposal}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Proposed architecture for implementing advanced error correction models for audiovisual inputs with CNNs}}{12}\protected@file@percent }
\newlabel{fig:proposal}{{15}{12}}
