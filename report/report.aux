\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Domain and Task}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Playing Field Arrangement}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual representation of the playing field}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Playing Rules}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Task Goal}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}State Transition and Reward Functions}{2}\protected@file@percent }
\newlabel{sec:transition}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Playing field with location ids}}{2}\protected@file@percent }
\newlabel{fig:field_indexed}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces State string with equivalent playing field representation}}{2}\protected@file@percent }
\newlabel{fig:field_mapping}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning Policy}{3}\protected@file@percent }
\newlabel{sec:learning_policy}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Graphical Representation \& R Matrix}{3}\protected@file@percent }
\newlabel{sec:r_matrix}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Base Transition Graph}}{3}\protected@file@percent }
\newlabel{fig:transition_graph}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces R Matrix generated for experiment with a single agent}}{4}\protected@file@percent }
\newlabel{fig:r_matrix}{{5}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Q-Learning Parameters}{4}\protected@file@percent }
\newlabel{sec:params}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discount Factor - $\gamma $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Learning Rate - $\alpha $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Epsilon - $\epsilon $}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Experiment duration - (\texttt  {num\_episodes} and \texttt  {max\_steps})}{4}\protected@file@percent }
\newlabel{sec:params:duration}{{5.4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Reward/Penalty}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Number of Agents - (\texttt  {num\_agents})}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Q-Matrix Update Cycle}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Initial Results}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Single agent}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Percentage of episodes able to reach terminal state in single agent experiment}}{6}\protected@file@percent }
\newlabel{fig:exp1:win_percent}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Steps taken per episode in single agent experiment}}{7}\protected@file@percent }
\newlabel{fig:exp1:steps}{{7}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of updates to Q Matrix per step taken in single agent experiment}}{7}\protected@file@percent }
\newlabel{fig:exp1:updates}{{8}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Two agents}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Percentage of episodes able to reach terminal state in two-agent experiment}}{7}\protected@file@percent }
\newlabel{fig:exp2:win_percent}{{9}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Further Tests}{7}\protected@file@percent }
\newlabel{sec:further_tests}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Steps taken per episode in two-agent experiment}}{8}\protected@file@percent }
\newlabel{fig:exp2:steps}{{10}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Number of updates to Q Matrix per step taken in two-agent experiment}}{8}\protected@file@percent }
\newlabel{fig:exp2:updates}{{11}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Experiment duration}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}$\alpha $ values}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Win \% progression with varying $\alpha $ - Single agent}}{8}\protected@file@percent }
\newlabel{fig:exp3:win_percent}{{12}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Win \% progression with varying $\alpha $ - Two agents}}{9}\protected@file@percent }
\newlabel{fig:exp3:win_percent_two}{{13}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}$\gamma $ values}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Win \% progression with varying $\gamma $ - Single agent}}{9}\protected@file@percent }
\newlabel{fig:exp4:win_percent}{{14}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Win \% progression with varying $\gamma $ - Two agents}}{9}\protected@file@percent }
\newlabel{fig:exp4:win_percent_two}{{15}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Maximum Episode duration - \texttt  {max\_steps}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}$\epsilon $ decay}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Win \% progression with varying \texttt  {max\_steps} - Single agent}}{10}\protected@file@percent }
\newlabel{fig:exp5:win_percent}{{16}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Win \% progression with varying \texttt  {max\_steps} - Two agents}}{10}\protected@file@percent }
\newlabel{fig:exp5:win_percent_two}{{17}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Quantitative Analysis}{10}\protected@file@percent }
\newlabel{sec:quant_analysis}{{9}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Qualitative Analysis - Single Agent}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Maximum Episode duration - \texttt  {max\_steps}}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Win \% progression with varying $\epsilon $ decay change threshold - Single agent}}{11}\protected@file@percent }
\newlabel{fig:exp6:change}{{18}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Win \% progression with varying $\epsilon $ decay rate - Single agent}}{11}\protected@file@percent }
\newlabel{fig:exp5:threshold}{{19}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}$\alpha $ and $\gamma $}{11}\protected@file@percent }
\newlabel{sec:quali:gamma}{{10.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Decay Rate - $\epsilon $}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Qualitative Analysis - Two Agents}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Advanced Reinforcement Learning Techniques}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Q-learning and Psychological Error Correction Models}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Error Correction Implementation Proposal}{12}\protected@file@percent }
